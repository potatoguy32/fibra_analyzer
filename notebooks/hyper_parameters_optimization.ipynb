{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of the box python modules\n",
    "import sqlite3\n",
    "import os\n",
    "import pathlib\n",
    "import math\n",
    "import json\n",
    "\n",
    "# Data wrangling stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, max_error\n",
    "\n",
    "# Neural network stuff\n",
    "from darts import TimeSeries, concatenate\n",
    "from darts.utils.callbacks import TFMProgressBar\n",
    "from darts.models import TCNModel, RNNModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.metrics import mape, r2_score, rmse, smape\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts.utils.likelihood_models import GaussianLikelihood, QuantileRegression, ExponentialLikelihood, GammaLikelihood\n",
    "from pytorch_lightning.callbacks import Callback, EarlyStopping\n",
    "\n",
    "# import lightning.pytorch as pl\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_contour,\n",
    "    plot_param_importances,\n",
    ")\n",
    "\n",
    "\n",
    "rc('mathtext', default='regular')\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "DATABASE_PATH = os.environ['DATABASE_PATH']\n",
    "MODEL_STORE_PATH = os.environ['MODEL_STORE']\n",
    "\n",
    "def generate_torch_kwargs():\n",
    "    # run torch models on CPU, and disable progress bars for all model stages except training.\n",
    "    return {\n",
    "        \"pl_trainer_kwargs\": {\n",
    "            \"accelerator\": \"cpu\",\n",
    "            \"callbacks\": [TFMProgressBar(enable_train_bar_only=True)],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(DATABASE_PATH) as conn:\n",
    "    cur = conn.cursor()\n",
    "    res = cur.execute(\"SELECT * FROM market_data where date >= '2020-09-01' order by ticker, date;\")\n",
    "    entries = res.fetchall()\n",
    "\n",
    "full_dataset = pd.DataFrame(entries, columns=['ticker', 'date', 'open', 'high', 'low', 'close', 'volume', 'dividends', 'stock_splits'])\n",
    "full_dataset['date'] = pd.to_datetime(full_dataset['date'])\n",
    "full_dataset['avg_price'] = full_dataset[['open', 'high', 'low', 'close']].mean(axis=1)\n",
    "full_dataset['paid_dividends'] = full_dataset['dividends'].replace(0, np.nan)\n",
    "full_dataset['period'] = full_dataset.date.dt.to_period('Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaPruning(PyTorchLightningPruningCallback, pl.Callback):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model_name = 'FirstTCN'\n",
    "    callbacks = [OptunaPruning(trial, monitor=\"val_loss\"), EarlyStopping(\"val_loss\", min_delta=0.001, patience=3, verbose=True)]\n",
    "    pl_trainer_kwargs = {\"callbacks\": callbacks}\n",
    "\n",
    "    # set input_chunk_length, between 5 and 14 days\n",
    "    days_in = trial.suggest_int(\"days_in\", 20, 22)\n",
    "\n",
    "    # set out_len, between 1 and 13 days (it has to be strictly shorter than in_len).\n",
    "    days_out = trial.suggest_int(\"days_out\", 19, days_in - 1)\n",
    "\n",
    "    # Other hyperparameters\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 8, 12)\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 5, 10)\n",
    "    weight_norm = trial.suggest_categorical(\"weight_norm\", [False, True])\n",
    "    likelihood = trial.suggest_categorical(\"likelihood\", [None, GaussianLikelihood(), QuantileRegression(), ExponentialLikelihood()])\n",
    "    dilation_base = trial.suggest_int(\"dilation_base\", 2, 4)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.25)\n",
    "    lr = trial.suggest_float(\"lr\", 0.01, 0.1, log=True)\n",
    "\n",
    "    # build and train the TCN model with these hyper-parameters:\n",
    "    model = TCNModel(\n",
    "        input_chunk_length=days_in,\n",
    "        output_chunk_length=days_out,\n",
    "        dropout=dropout,\n",
    "        dilation_base=dilation_base,\n",
    "        weight_norm=weight_norm,\n",
    "        kernel_size=kernel_size,\n",
    "        likelihood=likelihood,\n",
    "        num_filters=num_filters,\n",
    "        pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "        optimizer_kwargs={\"lr\": lr},\n",
    "        model_name=model_name,\n",
    "        save_checkpoints=True,\n",
    "        force_reset=True\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        series=train_scaled,\n",
    "        past_covariates=month_series,\n",
    "        val_series=val_scaled,\n",
    "        val_past_covariates=month_series,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    model = TCNModel.load_from_checkpoint(model_name=model_name, best=True)\n",
    "\n",
    "    # Evaluate how good it is on the validation set\n",
    "    preds = model.predict(series=train_scaled, n=len(val))\n",
    "    preds_inverse_scaled = scaler.inverse_transform(preds)\n",
    "    error = rmse(val, preds_inverse_scaled, verbose=True) # , n_jobs=-1\n",
    "    # smape_val = np.mean(smapes)\n",
    "\n",
    "    return error if error != np.nan else float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_callback(study, trial):\n",
    "    print(f\"Current value: {trial.value}, Current params: {trial.params}\")\n",
    "    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in full_dataset.ticker.unique():\n",
    "    ticker_name = ticker.split('.')[0]\n",
    "    sample = full_dataset.query(f\"ticker == '{ticker}'\")\n",
    "\n",
    "    training_data_len = math.ceil(len(sample) * 0.8)\n",
    "    train_data_date = sample.iloc[training_data_len, 1]\n",
    "\n",
    "    ts = TimeSeries.from_dataframe(df=sample, time_col='date', value_cols='avg_price', freq='B')\n",
    "    ts = fill_missing_values(ts, fill='auto')\n",
    "    train, val = ts.split_after(0.7)\n",
    "\n",
    "    scaler = Scaler()\n",
    "\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    val_scaled = scaler.transform(val)\n",
    "    ts_scaled = scaler.transform(ts)\n",
    "\n",
    "    month_series = datetime_attribute_timeseries(ts, attribute=\"month\", one_hot=True)\n",
    "\n",
    "\n",
    "    ## Optimize hyper parameters\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    study.optimize(objective, timeout=7200, callbacks=[print_callback])\n",
    "\n",
    "    # We could also have used a command as follows to limit the number of trials instead:\n",
    "    # study.optimize(objective, n_trials=100, callbacks=[print_callback])\n",
    "\n",
    "    # Finally, print the best value and best hyperparameters:\n",
    "    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")\n",
    "\n",
    "    with open(pathlib.Path(MODEL_STORE_PATH).joinpath(f'params/tcn_model_optim_params_{ticker_name}.json'), 'w') as f:\n",
    "        json.dump(study.best_trial.params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(study, params=[\"lr\", \"num_filters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fibra_analyzer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
